问题一：
1.批量导入的功能easyexcel多少秒导入5000，测试了多少数据，不能说是测试人员测的。
2.批量插入十万条数据,使用原生的MySQL的insert语句,java代码实现使用foreach循环1000次放入数据到java对象里,
再用MySQL原生语句一次性插入1000条.循环操作....
3.还有使用登录功能遇到的最大问题是什么？
4.JWT被盗取(注意没有被篡改过只是本来应该发给A的被B拿了),怎么防止B访问成功?使用非对称算法,生成公匙和私匙.

问题：
1.批量导入的功能easyexcel多少秒导入5000，测试了多少数据，不能说是测试人员测的。
2.还有使用登录功能遇到的最大问题是什么？

项目: 宜享家
1.如何参与技术选型: 开评审会,
首先是根据UI画出来的原型图和需求文档,确定好一些基本的字段, 再根据表表之间的关系, 比如用户表和房源表加上关系字段, 加上辅助字段is_delete, create_time, update_time,create_person. 再加上冗余字段,省去多表联查.
自己参与设计的库表有 房源表, 房源report表,房源report_config表, 房源report_content表, ap_behavior_entry表(行为实体), ap_follow_behavior关注表, ap_likes_behavior点赞表,
ap_read_behavior表,ap_comment表,ap_comment_reply表.

2.APP端房源Report讯息是上划下滑的, 每次往下滑是查询之前时间的report讯息, 每次往上划是查询最新的时间的report讯息, 这些讯息都是从MySQL数据库查出来的.
如何实现深度分页? 使用标签记录法(上一页最后的id或者时间来作为标记作为下一页的起点) 实现深度分页, 但是这样做无法跳页

3.使用SpringTask(实现简单,不用依赖XXL-JOB调度中心),打上@schedule注解,里面Corn=写七子表达式.每30秒进行一次检测. 
注入stringRedisTemple去查询key值为recordtime的值,有的话这说明是增量更新, 记录下时间更新到redis中, 然后根据之前redis记录的时间跟现在的时间做对比, 更新这个时间段内的所有的房源report更新到es中(不存在的会被新增, 存在的会被覆盖更新);

全量更新: 访问路径是importAll, 获取当前系统时间, 首先判断redis中有无key为record_time的value值,有说明已经全量更新过此时不进行,没有就查询数据库进行全量更新.
这个全量更新是使用异步线程来做的, 我们将查到的数据进行取模运算, 取模给它分成几个任务,new ThreadPoolExecutor自定义线程池,用计数器CountDownLatch等待所有的房源report同步完.

5.每次用户点击收藏关注, 收藏等行为时都会异步调用Kafka发送信息, 用户行为微服务端的kafka会监听这个消息,查询出对应的行为实体,将用户的行为记录到表中.

6.Kafka流式处理:
springTask的定时任务,凌晨四点进行筛选前5天的房源report, 按照权重计算分数值, 根据频道id将文章存储到各个频道对应redis的zset中并赋予分值, zset使其自动排序.(30篇文章)

实时热点房源report计算, 用户关注,收藏信息有kafka收集发送,同时Kafka流式处理器接收该信息进行聚合处理,等到达到一定量之后 将处理的信息计算权重(分值*3),再次发送信息,kafka监听到消息就会将对应的report存储到redis.(只存id,因为修改之后的关注数,收藏数已经变化,存所有的话,可能获取不到最新的内容)

7. es优化搜索的前缀树;

8.HomeUP用户发表提交,此时房源的状态就会被改变成已经提交状态,同时rabbitMQ异步调用发起审核, 对其敏感词过滤, 调用阿里云文本反垃圾, 阿里云图片反垃圾. 审核成功的话会使用Feign修改房源状态为待发布, 待复核的就修改状态为 待人工审核.
使用xxl-job定时任务(因为这个可以进行分片处理)每10秒检测一次数据库, 查出状态为自动审核或者人工审核通过, 发布时间(用户有设置定时发布)为当前时间之前或相等的,根据这个可以同步房源report信息更新三个表, 房源report,房源report_config,房源report_content,更新同步完成之后会使用Feign来改变房源的状态;

